- [x] Fix masking
- [x] Fix tokenizer
- [x] Positional encodings
- [x] Loss per token
- [x] `assert` every shape
- [x] fix loss calculation, use logits not softmax
- [ ] Batching

Loss graph for LAMBADA next word prediction      
<img width="1592" alt="Screenshot 2024-10-11 at 11 48 20â€¯AM" src="https://github.com/user-attachments/assets/5910c8fe-91ff-41ff-8058-32c848311486">
