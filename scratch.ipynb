{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be377f9-9eec-4983-bb3f-572bcf169939",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ece7ed-2963-44bc-9f8d-7ab3c1d83ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from fancy_einsum import einsum\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import wandb\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "wandb.login()\n",
    "wandb.init(\n",
    "    project=\"attention-is-all-you-need\",\n",
    "    config={\n",
    "        \"epochs\": num_epochs\n",
    "    }\n",
    ")\n",
    "\n",
    "training_data = load_dataset(\"wompzik/lambada\", split=\"train[:]\")\n",
    "\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "num_layers = 6\n",
    "seq_len = 512\n",
    "d_model = 512\n",
    "\n",
    "vocab_dim = 50257+1\n",
    "\n",
    "#TODO: change anywhere with (1, ) to support (batch_size, )\n",
    "\n",
    "class FFN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()            \n",
    "        \n",
    "        self.d_ff = d_model*4\n",
    "        self.W_1 = torch.nn.Linear(d_model, self.d_ff)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.W_2 = torch.nn.Linear(self.d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # assert x.shape == (seq_len, d_model)    \n",
    "        out = self.W_1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.W_2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class MHA(torch.nn.Module):\n",
    "    def __init__(self, n_heads=8, has_mask=False):  \n",
    "        super().__init__()          \n",
    "        \n",
    "        self.has_mask = has_mask\n",
    "        self.d_head = d_model // n_heads\n",
    "        \n",
    "        self.scale = 1/math.sqrt(self.d_head)\n",
    "\n",
    "        assert d_model == 512\n",
    "        assert self.d_head == 64\n",
    "\n",
    "        self.W_Q = torch.nn.Parameter(torch.nn.init.xavier_uniform_(torch.zeros(size=(d_model, n_heads, self.d_head))))\n",
    "        self.W_K = torch.nn.Parameter(torch.nn.init.xavier_uniform_(torch.zeros(size=(d_model, n_heads, self.d_head))))\n",
    "        self.W_V = torch.nn.Parameter(torch.nn.init.xavier_uniform_(torch.zeros(size=(d_model, n_heads, self.d_head))))\n",
    "        self.W_O = torch.nn.Parameter(torch.nn.init.xavier_uniform_(torch.zeros(size=(n_heads*self.d_head, d_model))))\n",
    "                \n",
    "    def attn(self, x_q, x_k, attention_mask):\n",
    "        Q = einsum('batch seq_len d_model, d_model n_heads d_head -> batch seq_len n_heads d_head', x_q, self.W_Q)\n",
    "        K = einsum('batch seq_len d_model, d_model n_heads d_head -> batch seq_len n_heads d_head', x_k, self.W_K)\n",
    "        V = einsum('batch seq_len d_model, d_model n_heads d_head -> batch seq_len n_heads d_head', x_k, self.W_V)\n",
    "\n",
    "        causal_mask = torch.ones(1, seq_len, seq_len).to(device) \n",
    "        if self.has_mask:\n",
    "            causal_mask = torch.tril(causal_mask)\n",
    "\n",
    "        attention_mask = torch.unsqueeze(attention_mask, dim=1)\n",
    "        causal_mask = causal_mask * attention_mask\n",
    "        \n",
    "        # softmax along dim=-1 of Q@K.T => seq_len\n",
    "        attn_scores = self.scale * einsum('batch seq_len n_heads d_head, batch seq_len_2 n_heads d_head -> batch seq_len seq_len_2', Q, K) #Q@K.transpose(-2,-1) # bcd,bdc -> bcc\n",
    "        # bcnh, bcnh -> bcc\n",
    "        attn_scores = attn_scores.masked_fill_(causal_mask == 0, -float('inf'))\n",
    "        sm = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        head = einsum('batch seq_len seq_len_2, batch seq_len n_heads d_head -> batch seq_len d_head', sm, V)\n",
    "        return head\n",
    "        # head = einsum('batch seq_len seq_len_2, batch seq_len n_heads d_head -> batch' sm, V)         # head = sm @ V\n",
    "\n",
    "    def forward(self, x_q, x_k, attention_mask):\n",
    "        assert x_q.shape == (1, seq_len, d_model)\n",
    "        assert x_k.shape == (1, seq_len, d_model)\n",
    "        \n",
    "        heads = torch.cat([self.attn(x_q, x_k, attention_mask) for _ in range(8)], dim=-1)\n",
    "        res = heads @ self.W_O\n",
    "        return res\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.M = MHA(has_mask=False)\n",
    "        self.F = FFN()\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        self.LN = torch.nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, attention_mask):\n",
    "        mha = self.M(x, x, attention_mask)\n",
    "        mha = self.dropout(mha)\n",
    "\n",
    "        sl1 = self.LN(x+mha)\n",
    "\n",
    "        ffn = self.F(sl1)\n",
    "        ffn = self.dropout(ffn)\n",
    "\n",
    "        sl2 = self.LN(sl1 + ffn)\n",
    "\n",
    "        return sl2\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.M1 = MHA(has_mask=True)\n",
    "        self.M2 = MHA(has_mask=False)\n",
    "        self.F = FFN()\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        self.LN = torch.nn.LayerNorm(d_model)        \n",
    "    \n",
    "    def forward(self, x, enc_out, attention_mask):\n",
    "        mmha = self.M1(x, x, attention_mask)\n",
    "        mmha = self.dropout(mmha)\n",
    "\n",
    "        sl1 = self.LN(x + mmha)\n",
    "\n",
    "        mha = self.M2(x_q=sl1, x_k=enc_out, attention_mask=attention_mask)\n",
    "        mha = self.dropout(mha)\n",
    "        \n",
    "        sl2 = self.LN(sl1 + mha)\n",
    "\n",
    "        ffn = self.F(sl2)\n",
    "        ffn = self.dropout(ffn)\n",
    "\n",
    "        sl3 = self.LN(sl2 + ffn)\n",
    "\n",
    "        return sl3\n",
    "\n",
    "class EncoderDecoder(torch.nn.Module):      \n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        self.encs = torch.nn.ModuleList([Encoder() for _ in range(num_layers)])\n",
    "        self.decs = torch.nn.ModuleList([Decoder() for _ in range(num_layers)])\n",
    "        self.pos_embed = PosEmbed()\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)        \n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        enc_out = x + self.pos_embed.fwd()\n",
    "        for enc in self.encs:\n",
    "            enc_out = enc(enc_out, attention_mask)\n",
    "\n",
    "        dec_out = enc_out + self.pos_embed.fwd()\n",
    "        dec_out = self.dropout(dec_out)        \n",
    "        \n",
    "        for dec in self.decs:\n",
    "            dec_out = dec(enc_out, dec_out, attention_mask)\n",
    "            \n",
    "        return dec_out   \n",
    "\n",
    "class Embed(torch.nn.Module):\n",
    "    def __init__(self, vocab_dim, d_model):\n",
    "        super().__init__()\n",
    "        self.emb = torch.nn.Embedding(num_embeddings=vocab_dim, embedding_dim=d_model, padding_idx=50257)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.emb(x)\n",
    "        return res\n",
    "\n",
    "class PosEmbed():\n",
    "    def __init__(self):\n",
    "        self.emb = torch.zeros(seq_len, d_model).to(device)\n",
    "\n",
    "        for pos in range(seq_len):\n",
    "            for i in range(d_model//2):\n",
    "                arg = pos/(10000**(2*i/d_model))\n",
    "                if i % 2 == 0:\n",
    "                    self.emb[pos, 2*i] = math.sin(arg)\n",
    "                else:\n",
    "                    self.emb[pos, 2*i+1] = math.cos(arg)\n",
    "\n",
    "    def fwd(self):   \n",
    "        return self.emb\n",
    "\n",
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "       super().__init__()\n",
    "       self.input_embed = Embed(vocab_dim=vocab_dim, d_model=d_model)\n",
    "       self.enc_dec = EncoderDecoder()       \n",
    "       self.linear = torch.nn.Linear(d_model, vocab_dim) # (seq_len, d_model) -> (seq_len, vocab_dim)\n",
    "       self.pos_embed = PosEmbed()\n",
    "       self.dropout = torch.nn.Dropout(p=0.1) \n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "       #Nit: torch.nn.Sequential\n",
    "\n",
    "       assert input_ids.shape == (1, seq_len)\n",
    "\n",
    "       out = self.input_embed(input_ids)\n",
    "\n",
    "       assert out.shape == (1, seq_len, d_model) # embedding  \n",
    "\n",
    "       out = self.dropout(out)\n",
    "    \n",
    "       out = self.enc_dec(out, attention_mask)  \n",
    "\n",
    "       assert out.shape == (1, seq_len, d_model) \n",
    "\n",
    "       out = self.linear(out)\n",
    "\n",
    "       assert out.shape == (1, seq_len, vocab_dim)\n",
    "\n",
    "       return out \n",
    "    \n",
    "       # Softmax over dim=1, seq_len, for every position\n",
    "       #softmax = torch.softmax(input=out, dim=1) \n",
    "       #return softmax\n",
    "\n",
    "\n",
    "model = Transformer().to(device)\n",
    "\n",
    "#Nit: tokenizer fork warning\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "losses = []\n",
    "\n",
    "#TODO: scheduler\n",
    "# warmup_steps = 4000\n",
    "# step_num = 1\n",
    "# lr = d_model**(-0.5)*min(step_num**(-0.5), step_num * warmup_steps**(-1.5))\n",
    "optim = torch.optim.Adam(params=model.parameters(), betas=(0.9,0.98), eps=10E-9)#, lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for sample in training_data:\n",
    "        optim.zero_grad()\n",
    "\n",
    "        input_tokens = tokenizer(sample['until_last'], truncation=True, padding='max_length', max_length=seq_len, return_tensors=\"pt\").to(device)\n",
    "        target_tokens = tokenizer(sample['last_word'], truncation=True, padding='max_length', max_length=seq_len, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        output_tokens = model(**input_tokens)\n",
    "\n",
    "        loss = criterion(output_tokens.transpose(-2,-1), target_tokens['input_ids'])\n",
    "        losses.append(loss)\n",
    "        wandb.log({\"loss\": loss})\n",
    "\n",
    "        loss.backward()        \n",
    "        optim.step()\n",
    "        \n",
    "    if epoch % 2 == 0:\n",
    "        print(f\"loss: {loss} @ epoch: {epoch}\")\n",
    "        sys.stdout.flush()        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
